"""Quality assessment tools for the OCR pipeline.

This module provides utilities to compute character error rates,
word error rates and heading detection metrics.  It also supports
exporting per‑page statistics to a CSV file and writing a Markdown
report with summary histograms.  These functions are intended to be
called from the `scripts` folder after running the pipeline to
generate a gold standard and evaluate against it.
"""

from __future__ import annotations

import csv
import statistics
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, List, Sequence, Tuple

import Levenshtein  # type: ignore
import matplotlib.pyplot as plt  # type: ignore


def compute_cer(pred: str, gold: str) -> float:
    """Compute character error rate (CER) as Levenshtein distance / |gold|."""
    pred = pred.replace("\n", "")
    gold = gold.replace("\n", "")
    if not gold:
        return 0.0 if not pred else 1.0
    dist = Levenshtein.distance(pred, gold)
    return dist / len(gold)


def compute_wer(pred: str, gold: str) -> float:
    """Compute word error rate (WER)."""
    pred_words = pred.split()
    gold_words = gold.split()
    if not gold_words:
        return 0.0 if not pred_words else 1.0
    dist = Levenshtein.distance(" ".join(pred_words), " ".join(gold_words))
    return dist / len(gold_words)


def evaluate_headings(pred: Sequence[str], gold: Sequence[str]) -> Tuple[float, float, float]:
    """Compute F1 score, precision and recall for heading detection.

    Headings are represented as sequences of tags ("H1", "H2", "H3", "P").
    Tags beginning with "H" are considered positives, others negatives.
    """
    tp = sum(1 for p, g in zip(pred, gold) if p.startswith("H") and g.startswith("H"))
    fp = sum(1 for p, g in zip(pred, gold) if p.startswith("H") and not g.startswith("H"))
    fn = sum(1 for p, g in zip(pred, gold) if not p.startswith("H") and g.startswith("H"))
    precision = tp / (tp + fp) if tp + fp > 0 else 0.0
    recall = tp / (tp + fn) if tp + fn > 0 else 0.0
    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0
    return f1, precision, recall


@dataclass
class PageStats:
    page: int
    conf: float
    chars: int
    used_psm: int
    embedded: bool
    preprocess: str
    crop_pct: float


def write_csv(stats: Iterable[PageStats], path: Path) -> None:
    """Write per‑page statistics to a CSV file."""
    with path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["page", "conf", "chars", "used_psm", "embedded", "preprocess", "crop_pct"])
        for s in stats:
            writer.writerow([s.page, f"{s.conf:.2f}", s.chars, s.used_psm, int(s.embedded), s.preprocess, s.crop_pct])


def write_report(cers: Sequence[float], wers: Sequence[float], f1: float, out_path: Path) -> None:
    """Write a Markdown report summarising OCR accuracy and heading F1."""
    md = []
    md.append("# OCR Quality Report\n")
    md.append(f"**Average CER:** {statistics.mean(cers):.3%}  ")
    md.append(f"**Average WER:** {statistics.mean(wers):.3%}  ")
    md.append(f"**Heading F1:** {f1:.3f}\n")
    # plot histogram of confidences if available
    # for demonstration we will not embed actual histograms to avoid binary output here
    md.append("\nConfidence and error histograms can be generated by the evaluation script.\n")
    out_path.write_text("\n".join(md), encoding="utf-8")